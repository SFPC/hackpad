# **Motoi's Project stack**

All of these ideas are based on playing around with...

1.  Learning computer graphics
2.  Learning computer vision

## 100 screen shots everyday 

![](https://hackpad-attachments.s3.amazonaws.com/hackpad.com_rDklkYeKels_p.81202_1384283912720_Screen%20Shot%202013-11-12%20at%20%E5%8D%88%E5%BE%8C2.17.51.png)

**Category**

Learning computer graphics

**Ideas **

Many screen shots which I took everyday while writing code.

Every screen shot is related to my questions on [Questions Programming for Graphics](https://sfpc.hackpad.com/lrzPpxSUTLN). These screen shots + questions will represent my process of studying computer graphics.

**Output**

*   (displaying on screen)
*   (flipbook)

42 inch printer at eyebeam for print. 

![](https://hackpad-attachments.s3.amazonaws.com/hackpad.com_h6mw9PRn1Zo_p.77239_1384540512157_undefined)

evan roth cache self portrait 

## Google image search everyday

![](https://hackpad-attachments.s3.amazonaws.com/hackpad.com_rDklkYeKels_p.81202_1384284037489_Screen%20Shot%202013-11-12%20at%20%E5%8D%88%E5%BE%8C2.20.26.png)

**Category**

Learning computer vision

**Ideas **

*

[](http://www.youtube.com/watch?v=ifZ2fjN8ohw)[http://www.youtube.com/watch?v=ifZ2fjN8ohw](http://www.youtube.com/watch?v=ifZ2fjN8ohw)

Inspired by [Search by Image, Recursively, Transparent PNG, #1](https://vimeo.com/34949864). This video consists of a movie of the one day in SFPC, and the similar images of every frame suggested by Google image search algorithm. I wrote a script to generate this video for just my curiosity about what we look like. And then, I was curious about the question "_How long does Google take to find the correct images for every frame if I upload all of the original image of these?_"

1.  This webpage (to be linked to project page) measures that how long Google takes to find the correct images for every frame
2.  According to Amit, when he scrolled this page very fast, he could see some similar lines in both columns. I couldn't see the pattern similarity, but probably we can figure this out using computer vision.

**Output**

*   (displaying on screen)
*   (flipbook)

## STARBUCKS Name card (iOS app)

**Category**

(none)

**Ideas**

My name is Motoi Shimizu, from Tokyo, Japan.

I like starbucks coffee.

I often go to starbucks to buy a coffee.

And then I get a coffee, and my new starbucks name.

![](https://hackpad-attachments.s3.amazonaws.com/hackpad.com_rDklkYeKels_p.81202_1384283427093_IMG_5522.png)

_Motoy, Motoi, Mothany, Malto...._

I don't care if they misspelled my name because they are of course not familiar with my name "Motoi", but it's a bit sad. Through my starbucks experience in Brooklyn, eventually I could understand the reason why some Asian people have their English name. 

The idea of this iOS application is creating your fancy starbucks name to avoid misspelling.

![](https://hackpad-attachments.s3.amazonaws.com/hackpad.com_rDklkYeKels_p.81202_1384287354975_IMG_5519.jpg)

(example: Ando - common japanese name >> Andy)

1.  Typing your name 
2.  Checking your geo location to decide which name list should be used for generating new name
3.  Parsing your name to International Phonetic Alphabet
4.  Returns the best matching name

**Output**

iOS App

## text lighter (iOS app)

![](https://hackpad-attachments.s3.amazonaws.com/hackpad.com_rDklkYeKels_p.81202_1384363578509_jim2jpg.jpg)

**Category**

Learning Computer graphics? 

**Ideas**

This idea came up after learning some technics of realtime effect for graphics in glsl.

*   An iOS app for generating a real light effect for the text, which canâ€™t be created programmatically
*   I want to use the real world as an effecter 
*   the next version of my iOS app which I made last year ([](http://www.youtube.com/watch?v=62GYZgr3KQg))[http://www.youtube.com/watch?v=62GYZgr3KQg](http://www.youtube.com/watch?v=62GYZgr3KQg))

![](https://hackpad-attachments.s3.amazonaws.com/hackpad.com_rDklkYeKels_p.81202_1384282710495_IMG_7517.jpeg)

1.  typing text on iOS devices
2.  sending svg file of the text shape to RPi
3.  drawing the text using XY plotter + LED
4.  taking a picture using long exposure
5.  sending back the picture to iOS device

**Output**

*   iOS App

## TumblrMachine!

**Category**

fabrication + rPi

**Ideas**

zach ragdoll + rPi

**Output**

## Definition of the Media Art Pose

![](https://hackpad-attachments.s3.amazonaws.com/hackpad.com_rDklkYeKels_p.81202_1384286816001_IMG_5538.JPG)

_Something might happens here, let's do the media art pose everyone! _

_-- 2013, Jonathan Dahan_

**Category**

Learning Computer Vision

**Ideas **

Probably the _Media Art Pose _might be the common pose for everyone who are interested in the digital art all over the world. When we visited a museum, we found something that looked like a sensor, but nothing happened. And then, some of the people started moving their hands above their heads. The pose and movement of their hand was so familiar. I saw the pose many times in cheesy installations like kinect + projector. 

Making a definition of this hand movement and collecting sample data to create a formula of the movement of your hand will be helpful for making and debugging your application.

Exhibition at Eyebeam is a good chance to collect data.

1.  Installing Kinect + screen which looks like something might happen
2.  Recording 
3.  Calculate the average of the motion

**Output**

*   (Formula - movement of hand)
*   (Object moving according to the formula)
*   (Recorded Kinect data)

*

[](http://vimeo.com/26192400)http://vimeo.com/26192400 

*   also about interaction with art and media. 
*   human body in space with objects (art/ object/ projection) 

thomas struth 

![](https://hackpad-attachments.s3.amazonaws.com/hackpad.com_h6mw9PRn1Zo_p.77239_1384535348010_undefined)

## Understanding misuse of my lenses

![](https://hackpad-attachments.s3.amazonaws.com/hackpad.com_rDklkYeKels_p.81202_1384366889908_IMG_7706.jpeg)
<undefined><li>
</li></undefined>

**Category**

Learning Computer Graphics + optical instrument

**Ideas **

Sony VCL-ECU1 (wide converter) is compatible with only Sony 20mm pancake lens. But when I put the wide converter on to Sony MEX-5 55mm lens, I found than it is a nice combination for taking an interesting foggy 

photo even though it was misused. It was really like a real fragment shader.
<undefined><li>
</li></undefined>

![](https://hackpad-attachments.s3.amazonaws.com/hackpad.com_rDklkYeKels_p.81202_1384287150466_misuse.png)

![](https://hackpad-attachments.s3.amazonaws.com/hackpad.com_rDklkYeKels_p.81202_1384287273911_Screen%20Shot%202013-11-12%20at%20%E5%8D%88%E5%BE%8C3.09.18.png)
<undefined><li>
</li></undefined>

I want to know what is happening in my lenses.

**Output **

*   fragment shader
*   movie which is like [NY, NY...?](http://www.youtube.com/watch?v=ztxuCv5-4D4)

## Flipbook recompose 

![](https://hackpad-attachments.s3.amazonaws.com/hackpad.com_rDklkYeKels_p.81202_1384366516909_animation.gif)

**Category**

Learning Computer Vision + Learning Computer Graphics

**Ideas **

*   scanning all pages of the flipbook
*   tracking some of the points
*   calculating vector for the tracking object
*   (draw something)

**Output**

*   (Animation)
*   (new flipbook)

## http://shaderbattle.com/ w/ Jason

![](https://hackpad-attachments.s3.amazonaws.com/hackpad.com_rDklkYeKels_p.81202_1384367151706_Screen%20Shot%202013-11-13%20at%20%E5%8D%88%E5%BE%8C1.25.20.png)

**Category**

Computer Graphics

**Ideas **

*   environment for glsl live coding battle

**Output**

*   OFApp which commit every changes of glsl code to github
*   website